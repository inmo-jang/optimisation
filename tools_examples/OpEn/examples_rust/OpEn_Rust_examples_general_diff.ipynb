{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpEn Rust Examples: with General Gradient Function\n",
    "\n",
    "In this example, we are going to use a function that can obtain the gradient of any given function. This sort of function was used in relaxed_ik rust version. Now, we are trying to use this approach for [the previous example that we implemented before](https://github.com/inmo-jang/optimisation_tutorial/blob/master/tools_examples/OpEn/examples_rust/OpEn_Rust_examples_obs_avoidance_simplified.ipynb).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation (Remind from the previous example)\n",
    "\n",
    "Minimise $$f(\\mathbf{u}) = u_2$$ \n",
    "<div style=\"text-align: right\"> (P1) </div>\n",
    "\n",
    "subject to \n",
    "\n",
    "$$ \\psi_{O}(\\mathbf{x}) =  [1 - (\\mathbf{u} - \\mathbf{c})^{\\top}(\\mathbf{u} - \\mathbf{c})]_{+} = 0$$\n",
    "<div style=\"text-align: right\"> (P1C) </div> \n",
    "\n",
    "$$ u_2 = p_1 \\cdot (u_1 - p_2)^2 + p_3$$\n",
    "<div style=\"text-align: right\"> (P2C) </div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpEn Implementation\n",
    "\n",
    "First, we need to import \"optimization_engine\". Also, we use \"nalgebra\" for linear algebra calculation. \n",
    "- In your local PC, it should be also declared in \"Cargo.toml\".\n",
    "- Instead, in this jupyter notebook, we need to have \"extern crate\" as follows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extern crate optimization_engine;\n",
    "use optimization_engine::{\n",
    "    alm::*,\n",
    "    constraints::*, panoc::*, *\n",
    "};\n",
    "extern crate nalgebra;\n",
    "use nalgebra::base::{*};\n",
    "// use nalgebra::base::{Matrix4, Matrix4x2, Matrix4x1};\n",
    "// use std::cmp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Master Class\n",
    "\n",
    "You should note that `AlmFactory` should have `f` and `df` being with `\n",
    "fn f(u: &[f64], cost: &mut f64) -> Result<(), SolverError>` and `fn df(u: &[f64], grad: &mut [f64]) -> Result<(), SolverError>`, respectively. It means that it could be nicer if we have a master class that can simply turn out `f` or `df` values. Such an architecture is used in [`relaxed_ik` rust version](https://github.com/uwgraphics/relaxed_ik/blob/dev/src/RelaxedIK_Rust/src/bin/lib/groove/objective_master.rs), which is a good example to be worth having a look. In this example as well, we are going to implement a problem master class as follows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub struct ProblemMaster{\n",
    "    p_obs: Matrix2x1<f64>, // Obstacle Position\n",
    "    p: Vec<f64> // parameters (slice)\n",
    "}\n",
    "\n",
    "impl ProblemMaster{\n",
    "    pub fn init(_p: Vec<f64>, _p_obs: Matrix2x1<f64>) -> Self {\n",
    "        let p = _p;\n",
    "        let p_obs = _p_obs;\n",
    "        Self{p, p_obs}            \n",
    "    }\n",
    "    \n",
    "    // Cost function\n",
    "    pub fn f_call(&self, u: &[f64]) -> f64{\n",
    "        let cost = u[1];\n",
    "        cost\n",
    "    }\n",
    "    \n",
    "    pub fn f(&self, u: &[f64], cost: &mut f64){\n",
    "        *cost = self.f_call(u);        \n",
    "    }\n",
    "    \n",
    "    // Gradient of the cost function\n",
    "    pub fn df(&self, u: &[f64], grad: &mut [f64]){\n",
    "        let mut f_0 = self.f_call(u);\n",
    "        \n",
    "        for i in 0..u.len() {\n",
    "            let mut u_h = u.to_vec();\n",
    "            u_h[i] += 0.000001;\n",
    "            let f_h = self.f_call(u_h.as_slice());\n",
    "            grad[i] = (-f_0 + f_h) / 0.000001;\n",
    "        }\n",
    "       \n",
    "    } \n",
    "    \n",
    "    // F1 Constraint\n",
    "    pub fn f1_call(&self, u: &[f64])-> Vec<f64> {\n",
    "        let mut f1u = vec![0.0; u.len()];    \n",
    "        f1u[0] = (1.0 - (u[0]-self.p_obs[(0,0)]).powi(2) - (u[1]-self.p_obs[(1,0)]).powi(2) ).max(0.0);\n",
    "        f1u[1] = self.p[0]*(u[0] - self.p[1]).powi(2) + self.p[2] - u[1];\n",
    "        return f1u;\n",
    "    }\n",
    "    \n",
    "    pub fn f1(&self, u: &[f64], f1u: &mut [f64]){\n",
    "        let mut f1u_vec = self.f1_call(u); \n",
    "        for i in 0..f1u_vec.len(){\n",
    "            f1u[i] = f1u_vec[i];\n",
    "        }\n",
    "    }    \n",
    "    \n",
    "    // Jacobian of F1\n",
    "    pub fn jf1_call(&self, u: &[f64])-> Matrix2<f64> {\n",
    "        let mut jf1 = Matrix2::new(0.0, 0.0,\n",
    "                              0.0, 0.0);\n",
    "        \n",
    "        let mut f1_0 = self.f1_call(u);\n",
    "\n",
    "        for i in 0..f1_0.len(){\n",
    "            for j in 0..u.len() {\n",
    "                let mut u_h = u.to_vec();\n",
    "                u_h[j] += 0.000001;\n",
    "                let f_h = self.f1_call(u_h.as_slice());\n",
    "                jf1[(i,j)] = (-f1_0[i] + f_h[i]) / 0.000001;\n",
    "            }                        \n",
    "        }\n",
    "\n",
    "        return jf1;        \n",
    "    } \n",
    "    \n",
    "    // Jacobian Product (JF_1^{\\top}*d)\n",
    "    pub fn f1_jacobian_product(&self, u: &[f64], d: &[f64], res: &mut [f64]){\n",
    "        let test = self.f1_call(u);\n",
    "        \n",
    "        let mut jf1_matrix = self.jf1_call(u);\n",
    "        if test[0] < 0.0{ // Outside the obstacle\n",
    "            jf1_matrix[(0,0)] = 0.0;\n",
    "            jf1_matrix[(0,1)] = 0.0;  \n",
    "        }          \n",
    "        \n",
    "        let mut d_matrix = Matrix2x1::new(0.0, 0.0);\n",
    "        for i in 0..d.len(){\n",
    "            d_matrix[(i,0)] = d[i];\n",
    "        }\n",
    "        \n",
    "        let mut res_matrix =  jf1_matrix.transpose()*d_matrix;\n",
    "        \n",
    "        res[0] = res_matrix[(0,0)];\n",
    "        res[1] = res_matrix[(1,0)];  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn main(_p: &[f64], _centre: &[f64]) {\n",
    "    /// ===========================================\n",
    "    let mut p_obs = Matrix2x1::new(0.0, 0.0);\n",
    "    for i in 0.._centre.len(){\n",
    "        p_obs[(i,0)] = _centre[i];\n",
    "    }\n",
    "    \n",
    "    let mut p: Vec<f64> = Vec::new();        \n",
    "    for i in 0.._p.len(){\n",
    "        p.push(_p[i]);\n",
    "    }\n",
    "    \n",
    "    let mut pm = ProblemMaster::init(p, p_obs);\n",
    "    \n",
    "    /// ===========================================\n",
    "    \n",
    "    let tolerance = 1e-5;\n",
    "    let nx = 2; // problem_size: dimension of the decision variables\n",
    "    let n1 = 2; // range dimensions of mappings F1\n",
    "    let n2 = 0; // range dimensions of mappings F2\n",
    "    let lbfgs_mem = 5; // memory of the LBFGS buffer\n",
    "    \n",
    "    // PANOCCache: All the information needed at every step of the algorithm\n",
    "    let panoc_cache = PANOCCache::new(nx, tolerance, lbfgs_mem);\n",
    "    \n",
    "    // AlmCache: A cache structure that contains all the data \n",
    "    // that make up the state of the ALM/PM algorithm\n",
    "    // (i.e., all those data that the algorithm updates)\n",
    "    let mut alm_cache = AlmCache::new(panoc_cache, n1, n2);\n",
    "\n",
    "    let set_c = Zero::new(); // Set C\n",
    "    let bounds = Ball2::new(None, 100.0); // Set U\n",
    "    let set_y = Ball2::new(None, 1e12);  // Set Y\n",
    "\n",
    "    // ============= \n",
    "    // Re-define the functions linked to user parameters\n",
    "    let f = |u: &[f64], cost: &mut f64| -> Result<(), SolverError> {\n",
    "        pm.f(u, cost);\n",
    "        Ok(())\n",
    "    };\n",
    "    \n",
    "    let df = |u: &[f64], grad: &mut [f64]| -> Result<(), SolverError> {\n",
    "        pm.df(u, grad);\n",
    "        Ok(())\n",
    "    };\n",
    "    \n",
    "    let f1 = |u: &[f64], f1u: &mut [f64]| -> Result<(), SolverError> {\n",
    "        pm.f1(u, f1u);\n",
    "        Ok(())\n",
    "    };    \n",
    "    \n",
    "    let f1_jacobian_product = |u: &[f64], d: &[f64], res: &mut [f64]| -> Result<(), SolverError> {\n",
    "        pm.f1_jacobian_product(u,d,res);\n",
    "        Ok(())\n",
    "    };      \n",
    "    // ==============\n",
    "    \n",
    "    // AlmFactory: Prepare function psi and its gradient \n",
    "    // given the problem data such as f, del_f and \n",
    "    // optionally F_1, JF_1, C, F_2\n",
    "    let factory = AlmFactory::new(\n",
    "        f, // Cost function\n",
    "        df, // Cost Gradient\n",
    "        Some(f1), // MappingF1\n",
    "        Some(f1_jacobian_product), // Jacobian Mapping F1 Trans\n",
    "        NO_MAPPING, // MappingF2\n",
    "        NO_JACOBIAN_MAPPING, // Jacobian Mapping F2 Trans\n",
    "        Some(set_c), // Constraint set\n",
    "        n2,\n",
    "    );\n",
    "\n",
    "    // Define an optimisation problem \n",
    "    // to be solved with AlmOptimizer\n",
    "    let alm_problem = AlmProblem::new(\n",
    "        bounds,\n",
    "        Some(set_c),\n",
    "        Some(set_y),\n",
    "        |u: &[f64], xi: &[f64], cost: &mut f64| -> Result<(), SolverError> {\n",
    "            factory.psi(u, xi, cost)\n",
    "        },\n",
    "        |u: &[f64], xi: &[f64], grad: &mut [f64]| -> Result<(), SolverError> {\n",
    "            factory.d_psi(u, xi, grad)\n",
    "        },\n",
    "        Some(f1),\n",
    "        NO_MAPPING,\n",
    "        n1,\n",
    "        n2,\n",
    "    );\n",
    "\n",
    "    let mut alm_optimizer = AlmOptimizer::new(&mut alm_cache, alm_problem)\n",
    "        .with_delta_tolerance(1e-5)\n",
    "        .with_max_outer_iterations(200)\n",
    "        .with_epsilon_tolerance(1e-6)\n",
    "        .with_initial_inner_tolerance(1e-2)\n",
    "        .with_inner_tolerance_update_factor(0.5)\n",
    "        .with_initial_penalty(100.0)\n",
    "        .with_penalty_update_factor(1.05)\n",
    "        .with_sufficient_decrease_coefficient(0.2)\n",
    "        .with_initial_lagrange_multipliers(&vec![5.0; n1]);\n",
    "\n",
    "    let mut u = vec![0.0; nx]; // Initial guess\n",
    "    let solver_result = alm_optimizer.solve(&mut u);\n",
    "    let r = solver_result.unwrap();\n",
    "    println!(\"\\n\\nSolver result : {:#.7?}\\n\", r);\n",
    "    println!(\"Solution u = {:#.6?}\", u);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "##### Case (1) : $\\mathbf{p} = [0.5, 0.0, 0.0]$; and $\\mathbf{c} = [0.0, 0.0]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solver result : AlmOptimizerStatus {\n",
      "    exit_status: Converged,\n",
      "    num_outer_iterations: 88,\n",
      "    num_inner_iterations: 1178,\n",
      "    last_problem_norm_fpr: 0.0000006,\n",
      "    lagrange_multipliers: Some(\n",
      "        [\n",
      "            5.0298889,\n",
      "            3.9940012,\n",
      "        ],\n",
      "    ),\n",
      "    solve_time: 1.2774560ms,\n",
      "    penalty: 5737.3563215,\n",
      "    delta_y_norm: 0.0425532,\n",
      "    f2_norm: 0.0000000,\n",
      "}\n",
      "\n",
      "Solution u = [\n",
      "    0.910177,\n",
      "    0.414219,\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main(&[0.5, 0.0, 0.0], &[0.0, 0.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case (2) : $\\mathbf{p} = [0.5, 0.0, 0.0]$; and $\\mathbf{c} = [-0.25, 0.0]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solver result : AlmOptimizerStatus {\n",
      "    exit_status: Converged,\n",
      "    num_outer_iterations: 73,\n",
      "    num_inner_iterations: 249,\n",
      "    last_problem_norm_fpr: 0.0000009,\n",
      "    lagrange_multipliers: Some(\n",
      "        [\n",
      "            5.0161011,\n",
      "            5.5350137,\n",
      "        ],\n",
      "    ),\n",
      "    solve_time: 426.3990000µs,\n",
      "    penalty: 2897.7548129,\n",
      "    delta_y_norm: 0.0137157,\n",
      "    f2_norm: 0.0000000,\n",
      "}\n",
      "\n",
      "Solution u = [\n",
      "    0.716497,\n",
      "    0.256679,\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main(&[0.5, 0.0, 0.0], &[-0.25, 0.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case (3) : $\\mathbf{p} = [0.5, -0.5, 0.0]$; and $\\mathbf{c} = [0.0, 0.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solver result : AlmOptimizerStatus {\n",
      "    exit_status: Converged,\n",
      "    num_outer_iterations: 60,\n",
      "    num_inner_iterations: 876,\n",
      "    last_problem_norm_fpr: 0.0000001,\n",
      "    lagrange_multipliers: Some(\n",
      "        [\n",
      "            5.0008478,\n",
      "            0.8368964,\n",
      "        ],\n",
      "    ),\n",
      "    solve_time: 951.4560000µs,\n",
      "    penalty: 1463.5630916,\n",
      "    delta_y_norm: 0.0116015,\n",
      "    f2_norm: 0.0000000,\n",
      "}\n",
      "\n",
      "Solution u = [\n",
      "    -0.992612,\n",
      "    0.121341,\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main(&[0.5, -0.5, 0.0], &[0.0, 0.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case (4) : $\\mathbf{p} = [0.1, 0.0, 0.0]$; and $\\mathbf{c} = [0.5, 0.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solver result : AlmOptimizerStatus {\n",
      "    exit_status: Converged,\n",
      "    num_outer_iterations: 97,\n",
      "    num_inner_iterations: 334,\n",
      "    last_problem_norm_fpr: 0.0000001,\n",
      "    lagrange_multipliers: Some(\n",
      "        [\n",
      "            5.0000000,\n",
      "            0.9943379,\n",
      "        ],\n",
      "    ),\n",
      "    solve_time: 663.6030000µs,\n",
      "    penalty: 9345.5488840,\n",
      "    delta_y_norm: 0.0297599,\n",
      "    f2_norm: 0.0000000,\n",
      "}\n",
      "\n",
      "Solution u = [\n",
      "    -0.499689,\n",
      "    0.024972,\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main(&[0.1, 0.0, 0.0], &[0.5, 0.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case (5) : $\\mathbf{p} = [-0.5, -0.5864, 0.0]$; and $\\mathbf{c} = [-\n",
    "3.0, 2.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Solver result : AlmOptimizerStatus {\n",
      "    exit_status: Converged,\n",
      "    num_outer_iterations: 15,\n",
      "    num_inner_iterations: 18,\n",
      "    last_problem_norm_fpr: 0.0000003,\n",
      "    lagrange_multipliers: Some(\n",
      "        [\n",
      "            5.0000000,\n",
      "            0.9999921,\n",
      "        ],\n",
      "    ),\n",
      "    solve_time: 44.3680000µs,\n",
      "    penalty: 179.5856326,\n",
      "    delta_y_norm: 0.0000141,\n",
      "    f2_norm: 0.0000000,\n",
      "}\n",
      "\n",
      "Solution u = [\n",
      "    -0.586364,\n",
      "    -0.000000,\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main(&[0.5, -0.5864, 0.0], &[-3.0, 2.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the circle constraint does not cover the minimum point of the cost function. Thus, the optimal value should be zero, where $u_1 = p_2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Compared with the previous example, the solver can converge in all the test cases (In the previous example, we had to tune initial guess and max_iteration number). \n",
    "\n",
    "- Also, solution times are less than those in the preivous example. \n",
    "\n",
    "- We do not need to manually implement the gradients of the cost function and the constraints. \n",
    "\n",
    "## Future work\n",
    "\n",
    "- Let's make the `ProblemMaster` have resizable parameters (e.g. not binding to `Matrix2x1`). \n",
    "\n",
    "- Let's implement a path planner based on Model Predictive Control **without consideration of the system dynamics (only concerning the path)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/4f1590278592d1039147b01c055c21ca"
  },
  "gist": {
   "data": {
    "description": "home/inmo/Jupyter/OpEn_Rust_example.ipynb",
    "public": true
   },
   "id": "4f1590278592d1039147b01c055c21ca"
  },
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
